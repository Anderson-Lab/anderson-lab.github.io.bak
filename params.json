{"name":"Anderson Data Science Research Lab","tagline":"Machine Learning + Big Data + Data Science","body":"<i>If you don’t work on important problems, it’s not likely that you’ll do important work.</i> — Richard Hamming\r\n\r\n<i>Geeks stay up all night disassembling the world so they can put it back together with new features. They tinker and fix things that aren't broken. Geeks abandon the world around them because they're busing soldering together a new one. They obsess and, in many cases, they suffer.</i> — Matthew Inman\r\n\r\n# <a href=\"http://anderson-lab.github.io/\">The Lab</a>\r\n\r\n<p align=\"justify\">\r\n<a href=\"http://anderson-lab.github.io/\"><img src=\"http://freyja.cs.cofc.edu/Paul-labs-logo.png\" alt=\"Data Science Research Lab\" height=\"100\" align=\"right\"  hspace=\"10px\"/></a>\r\nThe Anderson Data Science Research Lab specializes in applying data mining, machine learning, and artificial intelligence to the fields of bioinformatics, genomics, and metabolomics. We develop algorithms and software to tackle some of the most challenging and interesting data intensive problems in the life sciences. Our research interests include data science, big data, pattern analysis in high-dimensional data sets, evolutionary computation and optimization, machine learning, computational genomics, cloud computing, computational metabolomics, and eScience. We currently have multidisciplinary projects underway in metabolomics, human cognition, toxicology, marine biology, medical genomics, biomedical informatics, and marine genomics.\r\n</p>\r\n\r\n# Research Groups\r\n## Data Science Foundations\r\nThe Data Science Foundations Group researches fundamental problems and solutions for the general field of data science, specializing in machine learning, big data, pattern analysis in high-dimensional data sets, and deep learning. Two example ongoing projects are research into kernel approximation methods for supervised learning and distributed deep learning algorithms on Apache Spark.\r\n\r\n### Distributed Deep Learning with Apache Spark\r\nIt has been shown that training large models with deep learning techniques increases their performance and classification power. A group at Google pioneered two such algorithms: Downpour SGD, an asynchronous stochastic gradient descent method, and Sandblaster, a framework which is built upon the idea of distributed batch optimization. These algorithms improve training by having many model replicas while having a shared set of parameters hosted on an independent server. Additionally, each model replica is a Distbelief model, a set of machines that communicate in order to build even larger networks for classification. However, this framework has not been made available to the public and relies on the implementation of consistent and rapid communication and message-passing. There is a rise in the popularity of frameworks that simplify these communication mechanisms, most notably Apache Spark. Spark’s popularity is partially attributed to its seamless integration with other cluster frameworks, such as Hadoop, as well as its capability to act as a stand-alone cluster.  Additionally, Spark has been shown to adapt well to the parallelization of popular machine learning algorithms, which is shown through Spark’s native machine learning library. We have implemented the original system and set of algorithms in Python using XML remote procedure calls as the method of communication. We plan to compare the efficiency and ease-of-use of this system to a Spark implementation of the framework.  \r\n\r\n### Fast Food Elastic Net\r\nBowick et al extended the application of fast food (FF) kernel approximations to neural networks by creating a multilayer perceptron (MLP) which learned nonlinear FF feature transforms at each layer, providing an additional nonlinearity application in the MLP algorithm. They compared the performance of FF optimized NNs (FONNs), which optimize the FF parameters alongside the weight and bias parameters in the MLP training algorithm (such as backpropagation), against that of FF randomized NNs (FRNNs), which randomly generate the FF parameters without optimization, saving computational resources during training. Dai et al extended the model that Le and Smola and Rahimi and Recht constructed on top of NNs (creating a doubly non-linear NN) by creating the Doubly Stochastic Kernel Machine. We propose to apply FF to various machine learning algorithms which are traditionally non-kernel based. We compare the performance, on a large n dataset, of a support vector machine equipped with elastic net (SVEN) (which is computationally impractical on such datasets) against that of a novel model composed of the input patterns being transformed by the nonlinear FF feature transforms before being passed through an elastic net.\r\n\r\n## Bioinformatics\r\n\r\n## Biomedical Informatics\r\n\r\n## Computational Metabolomics\r\n\r\n## C2G2\r\n\r\n## Primary Investigator\r\nDr. Paul Anderson graduated in 2004 from Wright State University with a B.S. degree in Computer Engineering. He received his masters in Computer Science in 2006 and his Ph.D. in Computer Science & Engineering in June 2010. After graduation, Dr. Anderson was awarded a Consortium of Universities Research Fellowship to study as a Bioinformatics Research Scientist for the Air Force Research Laboratory (AFRL). Dr. Anderson has published 24+ peer-reviewed articles in the fields of genomics, computational intelligence, metabolomics, e-Science, bioinformatics, cloud computing, cancer informatics, and computer science and engineering education. At present, Paul is an assistant professor in the Computer Science Department at the College of Charleston. He is the director of the Data Science Program, the first such undergraduate program in the country. Dr. Anderson is the director of several specialized and complementary research groups: Charleston Computational Genomics Group (C2G2), Computational Metabolomics Group (CMG), Bioinformatics Research Group (BiRG), and the Data Science Research Group (DSRG). His research labs at the College of Charleston specialize in applying data mining, machine learning, and artificial intelligence to the fields of bioinformatics, genomics, cancer informatics, and metabolomics. His lab develops algorithms and software to tackle some of the most challenging and interesting data intensive problems in the life sciences. Dr. Anderson’s research interests include data science, big data, pattern analysis in high-dimensionality data sets, evolutionary computation and optimization, machine learning, computational genomics, cloud computing, computational metabolomics, and eScience. He currently has multidisciplinary projects underway in metabolomics, human cognition and fatigue, toxicology, marine biology, cancer informatics, and medical and marine genomics. Dr. Anderson is also the primary investigator for new Omics NSF Research Experience for Undergraduates at the College of Charleston (<a href=\"http://omics.cofc.edu\">http://omics.cofc.edu</a>).","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}